{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d7c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n",
    "input_dir = Path(\"../deberta-v2-3-fast-tokenizer\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "\n",
    "for filename in [\n",
    "    'tokenization_deberta_v2.py',\n",
    "    'tokenization_deberta_v2_fast.py',\n",
    "    \"deberta__init__.py\"]:\n",
    "    if str(filename).startswith(\"deberta\"):\n",
    "        filepath = deberta_v2_path/str(filename).replace(\"deberta\", \"\")\n",
    "    else:\n",
    "        filepath = deberta_v2_path/filename\n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "effdb823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.11.0\n",
      "transformers.__version__: 4.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 12:09:48.815774: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup,AdamW,AutoModel,AutoConfig\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "from transformers.models.deberta_v2 import DebertaV2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "788355cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# library\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, KFold,GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import logging\n",
    "from ast import literal_eval\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import ast\n",
    "import itertools\n",
    "from sklearn.metrics import f1_score,recall_score\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a373242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================\n",
    "# Constant\n",
    "# ==================\n",
    "\n",
    "FEATURES_PATH = \"../data/features.csv\"\n",
    "PATIENT_NOTES_PATH = \"../data/patient_notes.csv\"\n",
    "TRAIN_PATH = \"../data/train.csv\"\n",
    "TEST_PATH = \"../data/test.csv\"\n",
    "#PRETRAIN_MODEL_PATH = \"../output/pretrained/microsoft-deberta-v2-xlarge/microsoft-deberta-v2-xlarge-mlm-epoch-10.bin\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0653a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# ===============\n",
    "# Configs\n",
    "# ===============\n",
    "ex = \"051\"\n",
    "if not os.path.exists(f\"../output/exp/ex{ex}\"):\n",
    "    os.makedirs(f\"../output/exp/ex{ex}\")\n",
    "    os.makedirs(f\"../output/exp/ex{ex}/ex{ex}_model\")\n",
    "    \n",
    "OUTPUT_DIR = f\"../output/exp/ex{ex}\"\n",
    "MODEL_PATH_BASE = f\"../output/exp/ex{ex}/ex{ex}_model/ex{ex}\"\n",
    "LOGGER_PATH = f\"../output/exp/ex{ex}/ex{ex}.txt\"\n",
    "\n",
    "SEED = 0\n",
    "N_SPLITS = 5\n",
    "SHUFFLE = True\n",
    "num_workers = 4\n",
    "BATCH_SIZE = 4\n",
    "iters_to_accumulate = 1\n",
    "n_epochs = 6\n",
    "es_patience = 10\n",
    "max_len = 512\n",
    "weight_decay = 0.1\n",
    "beta = (0.9, 0.98)\n",
    "lr = 1e-5\n",
    "num_warmup_steps_rate = 0.1\n",
    "clip_grad_norm = 1.0\n",
    "\n",
    "MODEL_PATH = \"microsoft/deberta-v2-xlarge\"\n",
    "tokenizer = DebertaV2TokenizerFast.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "801312cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============\n",
    "# Settings\n",
    "# ===============\n",
    "oof_path1 = \"../output/exp/ex038/ex038_oof.npy\"\n",
    "oof_path2 = \"../output/exp/ex041/ex041_oof.npy\"\n",
    "oof_path3 = \"../output/exp/ex051/ex051_oof.npy\"\n",
    "nakama_path = '../output/nakama/ex141/ex141.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c862040e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 12:09:52,614 - INFO - logger set up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RootLogger root (DEBUG)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============\n",
    "# Functions\n",
    "# ===============\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def setup_logger(out_file=None, stderr=True, stderr_level=logging.INFO, file_level=logging.DEBUG):\n",
    "    LOGGER.handlers = []\n",
    "    LOGGER.setLevel(min(stderr_level, file_level))\n",
    "\n",
    "    if stderr:\n",
    "        handler = logging.StreamHandler(sys.stderr)\n",
    "        handler.setFormatter(FORMATTER)\n",
    "        handler.setLevel(stderr_level)\n",
    "        LOGGER.addHandler(handler)\n",
    "\n",
    "    if out_file is not None:\n",
    "        handler = logging.FileHandler(out_file)\n",
    "        handler.setFormatter(FORMATTER)\n",
    "        handler.setLevel(file_level)\n",
    "        LOGGER.addHandler(handler)\n",
    "\n",
    "    LOGGER.info(\"logger set up\")\n",
    "    return LOGGER\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield \n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "    \n",
    "LOGGER = logging.getLogger()\n",
    "FORMATTER = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "setup_logger(out_file=LOGGER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e90ffd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Main\n",
    "# ====================================================\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval)\n",
    "train['location'] = train['location'].apply(ast.literal_eval)\n",
    "features = pd.read_csv(FEATURES_PATH )\n",
    "def preprocess_features(features):\n",
    "    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "features = preprocess_features(features)\n",
    "patient_notes = pd.read_csv(PATIENT_NOTES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1b87a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[dad with recent heart attcak]</td>\n",
       "      <td>[696 724]</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>[mom with \"thyroid disease]</td>\n",
       "      <td>[668 693]</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>[chest pressure]</td>\n",
       "      <td>[203 217]</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>[intermittent episodes, episode]</td>\n",
       "      <td>[70 91, 176 183]</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[felt as if he were going to pass out]</td>\n",
       "      <td>[222 258]</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num  \\\n",
       "0  00016_000         0      16            0   \n",
       "1  00016_001         0      16            1   \n",
       "2  00016_002         0      16            2   \n",
       "3  00016_003         0      16            3   \n",
       "4  00016_004         0      16            4   \n",
       "\n",
       "                               annotation          location  \\\n",
       "0          [dad with recent heart attcak]         [696 724]   \n",
       "1             [mom with \"thyroid disease]         [668 693]   \n",
       "2                        [chest pressure]         [203 217]   \n",
       "3        [intermittent episodes, episode]  [70 91, 176 183]   \n",
       "4  [felt as if he were going to pass out]         [222 258]   \n",
       "\n",
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "257b1eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrect annotation\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfedea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['annotation_length'] = train['annotation'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bfb3ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    2860\n",
       "1    2860\n",
       "2    2860\n",
       "3    2860\n",
       "4    2860\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = GroupKFold(n_splits=N_SPLITS)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7141eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, \n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "def get_results_raw(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        if len(result) > 0:\n",
    "            if result[0] == 1:\n",
    "                result = np.concatenate([np.array([0]),result],axis=0)\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def get_results_pp(char_probs,txts,th=0.5,case=0):\n",
    "    results = []\n",
    "    for char_prob,txt in zip(char_probs,txts):\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        if len(result) > 0:\n",
    "            if result[0] == 1:\n",
    "                result = np.concatenate([np.array([0]),result],axis=0)\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result_ = []\n",
    "        if case != 9:\n",
    "            for r in result:\n",
    "                if r[0] >= 1:\n",
    "                    if txt[r[0] - 1] != ' ':\n",
    "                        result_.append([r[0]-1] + r)\n",
    "                    else:\n",
    "                        result_.append(r)\n",
    "                else:\n",
    "                    result_.append(r)\n",
    "            result_ = [f\"{min(r)} {max(r)}\" for r in result_]\n",
    "            result_ = \";\".join(result_)\n",
    "            results.append(result_)\n",
    "        else:\n",
    "            result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "            result = \";\".join(result)\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score,re_score = span_micro_f1(y_true, y_pred)\n",
    "    return score,re_score\n",
    "\n",
    "def span_micro_f1(truths,preds):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds), recall_score(truths, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8f151e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof38 = np.load(oof_path1)\n",
    "oof41 = np.load(oof_path2)\n",
    "oof51 = np.load(oof_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2c287fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nakama = pd.read_csv(nakama_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6bc2f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH1 = \"microsoft/deberta-v3-large\"\n",
    "MODEL_PATH2 = \"microsoft/deberta-v2-xlarge\"\n",
    "#MODEL_PATH3 = \"microsoft/deberta-v2-xlarge\"\n",
    "tokenizer1 = DebertaV2TokenizerFast.from_pretrained(MODEL_PATH1)\n",
    "tokenizer2 = DebertaV2TokenizerFast.from_pretrained(MODEL_PATH2)\n",
    "#tokenizer3 = AutoTokenizer.from_pretrained(MODEL_PATH3,trim_offsets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e83d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv\n",
    "valid_texts = train['pn_history'].values\n",
    "valid_labels = create_labels_for_scoring(train)\n",
    "char_probs1 = get_char_probs(valid_texts, oof38,  tokenizer1)\n",
    "char_probs2 = get_char_probs(valid_texts, oof41,  tokenizer2)\n",
    "char_probs3 = get_char_probs(valid_texts, oof51,  tokenizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93139108",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['feature_text'] = train['feature_text'].str.lower()\n",
    "train['pn_history'] = train['pn_history'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7a85538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14300"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nakama = nakama.sort_values(by=\"id\").reset_index(drop=True)\n",
    "sum(train[\"id\"] == nakama[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b08d6a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    num_workers=4\n",
    "    path=\"../input/nbme-debertav3large-exp141/\"\n",
    "    config_path=path+'config.pth'\n",
    "    model=\"microsoft/deberta-v3-large\"\n",
    "    batch_size=32\n",
    "    fc_dropout=0.\n",
    "    max_len=315\n",
    "    seed=42\n",
    "    n_fold=4\n",
    "    trn_fold=[0, 1, 2, 3]\n",
    "    losses=['bce', 'bce', 'bce', 'bce']\n",
    "    target_sizes=[1, 1, 1, 1]\n",
    "    \n",
    "char_probs_n = get_char_probs(nakama['pn_history'].values,\n",
    "                            nakama[[str(i) for i in range(CFG.max_len)]].values, \n",
    "                            tokenizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1e7a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 0.25\n",
    "w2 = 0.1\n",
    "w3 = 0.35\n",
    "w4 = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ef6f6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 0.25\n",
      "w2 0.1\n",
      "w3 0.35\n",
      "w4 0.3\n"
     ]
    }
   ],
   "source": [
    "w_sum = w1 + w2 + w3 + w4\n",
    "w1 /= w_sum\n",
    "w2 /= w_sum\n",
    "w3 /= w_sum\n",
    "w4 /= w_sum\n",
    "print(\"w1\",w1)\n",
    "print(\"w2\",w2)\n",
    "print(\"w3\",w3)\n",
    "print(\"w4\",w4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddc4e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(texts, preds):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    preds_pp = preds.copy()\n",
    "    tk0 = tqdm(range(len(preds_pp)), total=len(preds_pp))\n",
    "    for raw_idx in tk0:\n",
    "        pred = preds[raw_idx]\n",
    "        text = texts[raw_idx]\n",
    "        if len(pred) != 0:\n",
    "            # pp1: indexが1から始まる予測値は0から始まるように修正 ## +0.00123\n",
    "            if pred[0][0] == 1:\n",
    "                preds_pp[raw_idx][0][0] = 0\n",
    "            for p_index, pp in enumerate(pred):\n",
    "                start, end = pred[p_index]\n",
    "                if start == 0:\n",
    "                    break\n",
    "                # pp2: startとendが同じ予測値はstartを前に1ずらす ## +0.00012\n",
    "                if start == end:\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    break\n",
    "                # pp3: 始点が改行の場合始点を1つ後ろにずらす ## +0.00032\n",
    "                if text[start] == '\\n':\n",
    "                    preds_pp[raw_idx][p_index][0] = start + 1\n",
    "                    start = start + 1\n",
    "                # pp4: 1-2などは-2で予測されることがあるので修正 ## +0.00001\n",
    "                if text[start-1].isdigit() and text[start] == '-' and text[start+1].isdigit():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-1].isdigit() and text[start] == '/' and text[start+1].isdigit():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                # pp5: 67などは7で予測されることがあるので修正 ## +0.00001\n",
    "                if text[start-1].isdigit() and text[start].isdigit():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                # pp6: 文頭が大文字で始まるものは大文字部分が除かれて予測されることがあるので修正 ## +0.00013\n",
    "                if text[start-2] == '.' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-2] == ',' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-2] == ':' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                if text[start-2] == '-' and text[start-1].isupper():\n",
    "                    preds_pp[raw_idx][p_index][0] = start - 1\n",
    "                    start = start - 1\n",
    "                # pp7: heart -> h + eart となっているようなものを汎用的に修正する ## +0.00050\n",
    "                try:\n",
    "                    text_token = word_tokenize(text[start-1:end])\n",
    "                    first = text[start:end].split()[0]\n",
    "                    if first not in text_token:\n",
    "                        for t in text_token:\n",
    "                            if first == t[-len(first):]:\n",
    "                                sub = len(t) - len(first)\n",
    "                                preds_pp[raw_idx][p_index][0] = start - sub\n",
    "                                start = start - sub\n",
    "                                break\n",
    "                except:\n",
    "                    None\n",
    "                # pp8: .で終わっているもの ## 0.00001\n",
    "                if text[end-1:end] == '.':\n",
    "                    preds_pp[raw_idx][p_index][1] = end - 1\n",
    "                    end = end - 1\n",
    "    return preds_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d107933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14300/14300 [00:01<00:00, 12804.33it/s]\n",
      "2022-04-28 12:11:18,511 - INFO - cv:(0.894178945426973, 0.9077353554581247)\n"
     ]
    }
   ],
   "source": [
    "char_probs = []\n",
    "for i in range(len(char_probs1)):\n",
    "    char_probs.append(char_probs1[i] * w1 + char_probs2[i] * w2 + char_probs3[i] * w3 + char_probs_n[i] * w4 )\n",
    "results = get_results_raw(char_probs, th=0.47)\n",
    "preds = get_predictions(results)\n",
    "preds_pp = postprocess(valid_texts, preds)\n",
    "score = get_score(valid_labels, preds_pp)\n",
    "LOGGER.info(f'cv:{score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e73682d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 12:11:18,537 - INFO - Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-04-28 12:11:18,538 - INFO - NumExpr defaulting to 8 threads.\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 12238.12it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 12518.33it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 12319.60it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 12184.76it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 12275.67it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 12251.10it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 12296.88it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 12360.97it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 12374.01it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 12353.74it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 12350.69it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 12376.57it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 12497.41it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 13428.35it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 13137.83it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 12115.99it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 13193.15it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 13360.37it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 13159.59it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 13447.96it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 13266.27it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 13543.69it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 14089.03it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 13487.64it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 13556.79it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 14245.43it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 14843.20it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 14826.41it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 14906.69it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15031.16it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 14986.65it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 14987.78it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 14545.61it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15102.16it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15060.46it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15075.21it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 14959.67it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15150.39it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 14973.05it/s]\n",
      "100%|██████████| 1600/1600 [00:00<00:00, 14205.97it/s]\n",
      "100%|██████████| 1600/1600 [00:00<00:00, 14173.42it/s]\n",
      "100%|██████████| 1600/1600 [00:00<00:00, 14309.11it/s]\n",
      "100%|██████████| 1600/1600 [00:00<00:00, 14192.69it/s]\n",
      "100%|██████████| 1600/1600 [00:00<00:00, 14308.41it/s]\n",
      "100%|██████████| 1600/1600 [00:00<00:00, 14265.89it/s]\n",
      "100%|██████████| 1600/1600 [00:00<00:00, 14279.06it/s]\n",
      "100%|██████████| 1600/1600 [00:00<00:00, 14311.67it/s]\n",
      "100%|██████████| 1600/1600 [00:00<00:00, 14335.12it/s]\n",
      "100%|██████████| 1600/1600 [00:00<00:00, 14345.66it/s]\n",
      "100%|██████████| 1600/1600 [00:00<00:00, 14235.86it/s]\n",
      "100%|██████████| 1600/1600 [00:00<00:00, 14263.31it/s]\n",
      "100%|██████████| 1600/1600 [00:00<00:00, 14269.80it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 9525.41it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 9373.86it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 9409.63it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 9454.81it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 9298.52it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 9417.19it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 9525.88it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 9512.64it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 9334.78it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 9480.99it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 9474.20it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 9528.07it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 9534.98it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 11744.51it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 11751.07it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 11772.51it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 12630.21it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 11680.62it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 11780.72it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 11913.21it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 11957.17it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 11804.26it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 11244.25it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 11884.72it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 11795.91it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 11852.07it/s]\n",
      "100%|██████████| 1200/1200 [00:00<00:00, 13273.53it/s]\n",
      "100%|██████████| 1200/1200 [00:00<00:00, 11685.77it/s]\n",
      "100%|██████████| 1200/1200 [00:00<00:00, 11708.00it/s]\n",
      "100%|██████████| 1200/1200 [00:00<00:00, 11764.10it/s]\n",
      "100%|██████████| 1200/1200 [00:00<00:00, 11798.71it/s]\n",
      "100%|██████████| 1200/1200 [00:00<00:00, 11751.05it/s]\n",
      "100%|██████████| 1200/1200 [00:00<00:00, 11688.67it/s]\n",
      "100%|██████████| 1200/1200 [00:00<00:00, 11716.64it/s]\n",
      "100%|██████████| 1200/1200 [00:00<00:00, 11067.76it/s]\n",
      "100%|██████████| 1200/1200 [00:00<00:00, 11783.05it/s]\n",
      "100%|██████████| 1200/1200 [00:00<00:00, 11723.49it/s]\n",
      "100%|██████████| 1200/1200 [00:00<00:00, 11761.30it/s]\n",
      "100%|██████████| 1200/1200 [00:00<00:00, 11693.72it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 10440.86it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 10634.91it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 10389.71it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 12084.62it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 10479.91it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 10551.91it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 10544.02it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 10479.71it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 6043.24it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 5992.40it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 6058.21it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 5459.72it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 9190.56it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 14006.54it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 13931.71it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 14728.25it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 14019.38it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 14003.86it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 14125.78it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 14167.05it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 14047.82it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 14062.00it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 14186.43it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 14815.51it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 14174.97it/s]\n",
      "100%|██████████| 1800/1800 [00:00<00:00, 15043.80it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15783.45it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15758.27it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15853.85it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15727.19it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15743.41it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15924.99it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 17055.77it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15793.87it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15736.36it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15700.22it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15641.19it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15790.55it/s]\n",
      "100%|██████████| 1700/1700 [00:00<00:00, 15751.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "best_dict = {}\n",
    "for c in range(10):\n",
    "    for th in [0.47,0.475,0.48,0.485,0.49,0.495,0.50,0.505,0.51,0.515,0.52,0.525,0.53]:\n",
    "        valid_labels_ = np.array(valid_labels)[train[\"case_num\"] == c]\n",
    "        char_probs_ = np.array(char_probs)[train[\"case_num\"] == c]\n",
    "        valid_texts_ = valid_texts[train[\"case_num\"] == c]\n",
    "        results = get_results_raw(char_probs_,th=th)\n",
    "        preds = get_predictions(results)\n",
    "        preds_pp = postprocess(valid_texts_, preds)\n",
    "        score,re_score = get_score(valid_labels_, preds_pp)\n",
    "        #LOGGER.info(f'{th} case{i} cv:{score}')\n",
    "        if th == 0.47:\n",
    "            best_dict[c] = [th,score]\n",
    "        else:\n",
    "            if best_dict[c][1] < score:\n",
    "                best_dict[c] = [th,score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2047594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0.49, 0.8976772190005388],\n",
       " 1: [0.525, 0.9064990886585599],\n",
       " 2: [0.475, 0.8517623923219974],\n",
       " 3: [0.485, 0.9260359498514676],\n",
       " 4: [0.53, 0.9248769561757685],\n",
       " 5: [0.47, 0.8364175195561528],\n",
       " 6: [0.47, 0.906836587356394],\n",
       " 7: [0.53, 0.8801618303571429],\n",
       " 8: [0.485, 0.9256547241005716],\n",
       " 9: [0.53, 0.9290001463914508]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1cb8a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_raw_pp(char_probs, case_nums,th_dict):\n",
    "    results = []\n",
    "    for char_prob,case_num in zip(char_probs,case_nums):\n",
    "        result = np.where(char_prob >= th_dict[case_num][0])[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "240aa565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14300/14300 [00:01<00:00, 12912.26it/s]\n",
      "2022-04-28 12:12:21,010 - INFO - cv:(0.8946205091478651, 0.9030701138485383)\n"
     ]
    }
   ],
   "source": [
    "char_probs = []\n",
    "for i in range(len(char_probs1)):\n",
    "    char_probs.append(char_probs1[i] * w1 + char_probs2[i] * w2 + char_probs3[i] * w3 + char_probs_n[i] * w4 )\n",
    "case_nums = train[\"case_num\"].values\n",
    "results = get_results_raw_pp(char_probs, case_nums,best_dict)\n",
    "preds = get_predictions(results)\n",
    "preds_pp = postprocess(valid_texts, preds)\n",
    "score = get_score(valid_labels, preds_pp)\n",
    "LOGGER.info(f'cv:{score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
