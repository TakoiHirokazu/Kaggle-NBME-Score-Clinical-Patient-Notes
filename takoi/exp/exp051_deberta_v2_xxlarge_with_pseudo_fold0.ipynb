{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9012f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n",
    "input_dir = Path(\"../deberta-v2-3-fast-tokenizer\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "\n",
    "for filename in [\n",
    "    'tokenization_deberta_v2.py',\n",
    "    'tokenization_deberta_v2_fast.py',\n",
    "    \"deberta__init__.py\"]:\n",
    "    if str(filename).startswith(\"deberta\"):\n",
    "        filepath = deberta_v2_path/str(filename).replace(\"deberta\", \"\")\n",
    "    else:\n",
    "        filepath = deberta_v2_path/filename\n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090d8555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.11.0\n",
      "transformers.__version__: 4.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 21:52:29.778266: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup,AdamW,AutoModel,AutoConfig\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "from transformers.models.deberta_v2 import DebertaV2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9f57cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# library\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, KFold,GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import logging\n",
    "from ast import literal_eval\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import ast\n",
    "import itertools\n",
    "from sklearn.metrics import f1_score,recall_score\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c160a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================\n",
    "# Constant\n",
    "# ==================\n",
    "FEATURES_PATH = \"../data/features.csv\"\n",
    "PATIENT_NOTES_PATH = \"../data/patient_notes.csv\"\n",
    "TRAIN_PATH = \"../data/train.csv\"\n",
    "TEST_PATH = \"../data/test.csv\"\n",
    "PRETRAIN_MODEL_PATH = \"../output/pretrained/microsoft-deberta-v2-xxlarge/microsoft-deberta-v2-xxlarge-mlm-epoch-4.bin\"\n",
    "PSEUDO_DATA_PATH = \"../data/pseudo_plain.pkl\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IDS_SEQ_PATH = \"../output/deberta_v2_xlarge_ids.npy\"\n",
    "TOKEN_TYPE_PATH = \"../output/deberta_v2_xlarge_token_type.npy\"\n",
    "LABEL_PATH = \"../output/deberta_v2_xlarge_labels.npy\"\n",
    "MASK_PATH = \"../output/deberta_v2_xlarge_mask_seq.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b31fe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# ===============\n",
    "# Configs\n",
    "# ===============\n",
    "ex = \"051\"\n",
    "pseudo_exp  =\"032\"\n",
    "if not os.path.exists(f\"../output/exp/ex{ex}\"):\n",
    "    os.makedirs(f\"../output/exp/ex{ex}\")\n",
    "    os.makedirs(f\"../output/exp/ex{ex}/ex{ex}_model\")\n",
    "    \n",
    "OUTPUT_DIR = f\"../output/exp/ex{ex}\"\n",
    "MODEL_PATH_BASE = f\"../output/exp/ex{ex}/ex{ex}_model/ex{ex}\"\n",
    "LOGGER_PATH = f\"../output/exp/ex{ex}/ex{ex}.txt\"\n",
    "\n",
    "SEED = 0\n",
    "N_SPLITS = 5\n",
    "SHUFFLE = True\n",
    "num_workers = 4\n",
    "BATCH_SIZE = 3\n",
    "iters_to_accumulate = 2\n",
    "n_epochs = 3\n",
    "es_patience = 10\n",
    "max_len = 512\n",
    "weight_decay = 0.1\n",
    "beta = (0.9, 0.98)\n",
    "lr = 8e-6\n",
    "num_warmup_steps_rate = 0.1\n",
    "clip_grad_norm = 1.0\n",
    "pseudo_size = 100000\n",
    "eval_step = 19000\n",
    "MODEL_PATH = \"microsoft/deberta-v2-xxlarge\"\n",
    "tokenizer = DebertaV2TokenizerFast.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ec6077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.save_pretrained(\"../output/pretrained/microsoft-deberta-v3-large/mlm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c5d3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 21:52:34,208 - INFO - logger set up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RootLogger root (DEBUG)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============\n",
    "# Functions\n",
    "# ===============\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def setup_logger(out_file=None, stderr=True, stderr_level=logging.INFO, file_level=logging.DEBUG):\n",
    "    LOGGER.handlers = []\n",
    "    LOGGER.setLevel(min(stderr_level, file_level))\n",
    "\n",
    "    if stderr:\n",
    "        handler = logging.StreamHandler(sys.stderr)\n",
    "        handler.setFormatter(FORMATTER)\n",
    "        handler.setLevel(stderr_level)\n",
    "        LOGGER.addHandler(handler)\n",
    "\n",
    "    if out_file is not None:\n",
    "        handler = logging.FileHandler(out_file)\n",
    "        handler.setFormatter(FORMATTER)\n",
    "        handler.setLevel(file_level)\n",
    "        LOGGER.addHandler(handler)\n",
    "\n",
    "    LOGGER.info(\"logger set up\")\n",
    "    return LOGGER\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield \n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "    \n",
    "LOGGER = logging.getLogger()\n",
    "FORMATTER = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "setup_logger(out_file=LOGGER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbf1ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Main\n",
    "# ====================================================\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval)\n",
    "train['location'] = train['location'].apply(ast.literal_eval)\n",
    "features = pd.read_csv(FEATURES_PATH )\n",
    "def preprocess_features(features):\n",
    "    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "features = preprocess_features(features)\n",
    "patient_notes = pd.read_csv(PATIENT_NOTES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c359eab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[dad with recent heart attcak]</td>\n",
       "      <td>[696 724]</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>[mom with \"thyroid disease]</td>\n",
       "      <td>[668 693]</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>[chest pressure]</td>\n",
       "      <td>[203 217]</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>[intermittent episodes, episode]</td>\n",
       "      <td>[70 91, 176 183]</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[felt as if he were going to pass out]</td>\n",
       "      <td>[222 258]</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num  \\\n",
       "0  00016_000         0      16            0   \n",
       "1  00016_001         0      16            1   \n",
       "2  00016_002         0      16            2   \n",
       "3  00016_003         0      16            3   \n",
       "4  00016_004         0      16            4   \n",
       "\n",
       "                               annotation          location  \\\n",
       "0          [dad with recent heart attcak]         [696 724]   \n",
       "1             [mom with \"thyroid disease]         [668 693]   \n",
       "2                        [chest pressure]         [203 217]   \n",
       "3        [intermittent episodes, episode]  [70 91, 176 183]   \n",
       "4  [felt as if he were going to pass out]         [222 258]   \n",
       "\n",
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c09d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrect annotation\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65df7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['annotation_length'] = train['annotation'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c7dae54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    2860\n",
       "1    2860\n",
       "2    2860\n",
       "3    2860\n",
       "4    2860\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = GroupKFold(n_splits=N_SPLITS)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef6238ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(text, feature_text,tokenizer):\n",
    "    inputs =  tokenizer.encode_plus(text, feature_text, \n",
    "                           add_special_tokens=True,\n",
    "                           max_length=max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def create_label(text, annotation_length, location_list,tokenizer):\n",
    "    encoded = tokenizer.encode_plus(text,\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=max_len,\n",
    "                            padding='max_length',\n",
    "                            return_offsets_mapping=True)\n",
    "    offset_mapping = encoded['offset_mapping']\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    label[ignore_idxes] = -1\n",
    "    if annotation_length != 0:\n",
    "        for location in location_list:\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                for idx in range(len(offset_mapping)):\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                        start_idx = idx - 1\n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                        end_idx = idx + 1\n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    label[start_idx:end_idx] = 1\n",
    "    return label\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer):\n",
    "        self.feature_texts = df['feature_text'].values\n",
    "        self.pn_historys = df['pn_history'].values\n",
    "        self.annotation_lengths = df['annotation_length'].values\n",
    "        self.locations = df['location'].values\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.pn_historys[item], \n",
    "                               self.feature_texts[item],\n",
    "                               self.tokenizer)\n",
    "        label = create_label(self.pn_historys[item], \n",
    "                             self.annotation_lengths[item], \n",
    "                             self.locations[item],\n",
    "                             self.tokenizer)\n",
    "        return {\n",
    "              'token': torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n",
    "              'mask': torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n",
    "              'token_type':torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long),\n",
    "              \"y\":torch.tensor(label, dtype=torch.float32)\n",
    "               }\n",
    "    \n",
    "class TrainDataset_pseudo(Dataset):\n",
    "    def __init__(self,ids,mask,token_type,label,tokenizer):\n",
    "        self.ids = ids\n",
    "        self.mask = mask\n",
    "        self.token_type = token_type\n",
    "        self.label = label\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = {\"input_ids\":self.ids[item],\n",
    "                 \"attention_mask\":self.mask[item],\n",
    "                 \"token_type_ids\":self.token_type[item]}\n",
    "        label = self.label[item]\n",
    "        return {\n",
    "              'token': torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n",
    "              'mask': torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n",
    "              'token_type':torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long),\n",
    "              \"y\":torch.tensor(label, dtype=torch.float32)\n",
    "               }\n",
    "    \n",
    "class TrainDataset_pseudo_aug(Dataset):\n",
    "    def __init__(self,ids,mask,token_type,label,tokenizer,mask_aug_p,mask_ratio):\n",
    "        self.ids = ids\n",
    "        self.mask = mask\n",
    "        self.token_type = token_type\n",
    "        self.label = label\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_aug_p = mask_aug_p\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def mask_augment(self, inputs):\n",
    "        all_inds = np.arange(1, len(inputs[\"input_ids\"]) - 1)\n",
    "        n_mask = max(int(len(all_inds) * self.mask_ratio), 1)\n",
    "        np.random.shuffle(all_inds)\n",
    "        mask_inds = all_inds[:n_mask]\n",
    "        sep_ind = np.where(np.array(inputs[\"input_ids\"]) == 2)[0]\n",
    "        mask_inds = np.array([i for i in mask_inds if i < sep_ind[0]])\n",
    "        inputs_ids = np.array(inputs[\"input_ids\"])\n",
    "        try:\n",
    "            inputs_ids[mask_inds] = tokenizer.mask_token_id\n",
    "            inputs[\"input_ids\"] = list(inputs_ids)\n",
    "            return inputs\n",
    "        except:\n",
    "            return inputs\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        inputs = {\"input_ids\":self.ids[item],\n",
    "                 \"attention_mask\":self.mask[item],\n",
    "                 \"token_type_ids\":self.token_type[item]}\n",
    "        if float(torch.rand(1)) < self.mask_aug_p:\n",
    "            inputs = self.mask_augment(inputs)\n",
    "        label = self.label[item]\n",
    "        return {\n",
    "              'token': torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n",
    "              'mask': torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n",
    "              'token_type':torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long),\n",
    "              \"y\":torch.tensor(label, dtype=torch.float32)\n",
    "               }\n",
    "    \n",
    "class TrainDataset_aug(Dataset):\n",
    "    def __init__(self,df,tokenizer,mask_aug_p,mask_ratio):\n",
    "        self.feature_texts = df['feature_text'].values\n",
    "        self.pn_historys = df['pn_history'].values\n",
    "        self.annotation_lengths = df['annotation_length'].values\n",
    "        self.locations = df['location'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_aug_p = mask_aug_p\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts)\n",
    "    \n",
    "    def mask_augment(self, inputs):\n",
    "        all_inds = np.arange(1, len(inputs[\"input_ids\"]) - 1)\n",
    "        n_mask = max(int(len(all_inds) * self.mask_ratio), 1)\n",
    "        np.random.shuffle(all_inds)\n",
    "        mask_inds = all_inds[:n_mask]\n",
    "        sep_ind = np.where(np.array(inputs[\"input_ids\"]) == 2)[0]\n",
    "        mask_inds = np.array([i for i in mask_inds if i < sep_ind[0]])\n",
    "        inputs_ids = np.array(inputs[\"input_ids\"])\n",
    "        inputs_ids[mask_inds] = tokenizer.mask_token_id\n",
    "        inputs[\"input_ids\"] = list(inputs_ids)\n",
    "        return inputs\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.pn_historys[item], \n",
    "                               self.feature_texts[item],\n",
    "                               self.tokenizer)\n",
    "        label = create_label(self.pn_historys[item], \n",
    "                             self.annotation_lengths[item], \n",
    "                             self.locations[item],\n",
    "                             self.tokenizer)\n",
    "        if float(torch.rand(1)) < self.mask_aug_p:\n",
    "            inputs = self.mask_augment(inputs)\n",
    "        return {\n",
    "              'token': torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n",
    "              'mask': torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n",
    "              'token_type':torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long),\n",
    "              \"y\":torch.tensor(label, dtype=torch.float32)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dabac6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, \n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        if len(result) > 0:\n",
    "            if result[0] == 1:\n",
    "                result = np.concatenate([np.array([0]),result],axis=0)\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def get_results_pp(char_probs,txts,th=0.5,case=0):\n",
    "    results = []\n",
    "    for char_prob,txt in zip(char_probs,txts):\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        if len(result) > 0:\n",
    "            if result[0] == 1:\n",
    "                result = np.concatenate([np.array([0]),result],axis=0)\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result_ = []\n",
    "        if case != 9:\n",
    "            for r in result:\n",
    "                if r[0] >= 1:\n",
    "                    if txt[r[0] - 1] != ' ':\n",
    "                        result_.append([r[0]-1] + r)\n",
    "                    else:\n",
    "                        result_.append(r)\n",
    "                else:\n",
    "                    result_.append(r)\n",
    "            result_ = [f\"{min(r)} {max(r)}\" for r in result_]\n",
    "            result_ = \";\".join(result_)\n",
    "            results.append(result_)\n",
    "        else:\n",
    "            result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "            result = \";\".join(result)\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score,re_score = span_micro_f1(y_true, y_pred)\n",
    "    return score,re_score\n",
    "\n",
    "def span_micro_f1(truths,preds):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds), recall_score(truths, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70a6bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(custom_model, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            MODEL_PATH, \n",
    "        )\n",
    "        self.config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.ln1 = nn.LayerNorm(1536)\n",
    "        self.linear1 = nn.Linear(1536,768)\n",
    "        self.ln2 = nn.LayerNorm(768)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.linear2 = nn.Linear(768,1)\n",
    "        self._init_weights(self.linear1)\n",
    "        self._init_weights(self.linear2)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "            \n",
    "    def forward(self, ids, mask,token_type):\n",
    "        # pooler\n",
    "        emb = self.model(ids, mask,token_type)[\"last_hidden_state\"]\n",
    "        output = self.ln1(emb)\n",
    "        output = self.dropout1(output)\n",
    "        output = self.linear1(output)\n",
    "        output = self.ln2(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout2(output)\n",
    "        output = self.linear2(output)\n",
    "        return output\n",
    "    \n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, reduction='none', alpha=1, gamma=2):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        loss = self.alpha * (1. - pt)**self.gamma * bce_loss\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class SmoothFocalLoss(nn.Module):\n",
    "    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.focal_loss = FocalLoss(reduction='none', alpha=alpha, gamma=gamma)\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothFocalLoss._smooth(targets, self.smoothing)\n",
    "        loss = self.focal_loss(inputs, targets)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "    \n",
    "def collate(d,train=True):\n",
    "    mask_len = int(d[\"mask\"].sum(axis=1).max())\n",
    "    if train:\n",
    "        return {\"token\" : d['token'][:,:mask_len],\n",
    "                 \"mask\" : d['mask'][:,:mask_len],\n",
    "                 \"y\" : d['y'][:,:mask_len],\n",
    "                 \"token_type\" : d['token_type'][:,:mask_len],\n",
    "                  \"max_len\" : mask_len}\n",
    "    else:\n",
    "        return {\"token\" : d['token'][:,:mask_len],\n",
    "                 \"mask\" : d['mask'][:,:mask_len],\n",
    "                 \"token_type\" : d['token_type'][:,:mask_len],\n",
    "                  \"max_len\" : mask_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26bc4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_seq = np.load(IDS_SEQ_PATH)\n",
    "token_type_seq = np.load(TOKEN_TYPE_PATH)\n",
    "labels_seq = np.load(LABEL_PATH)\n",
    "mask_seq = np.load(MASK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01c69740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(PSEUDO_DATA_PATH, 'rb') as f:\n",
    "    pseudo_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c1e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# train\n",
    "# ================================\n",
    "with timer(\"deberta_v2_xxlarge\"):\n",
    "    set_seed(SEED)\n",
    "    oof_pred = np.ndarray((len(train),max_len))\n",
    "    for fold in range(1):\n",
    "        print(f\"===============fold{fold}:start===============\")\n",
    "        pseudo_label = np.load(f\"../output/exp/ex{pseudo_exp}/pseudo_fold{fold}_oof.npy\")\n",
    "        pseudo_label = pseudo_label > 0.5\n",
    "        pseudo_label = pseudo_label.astype(int)\n",
    "        pseudo_label = pseudo_label.reshape(-1,512)\n",
    "        pseudo_ids = np.load(f\"../output/exp/ex{pseudo_exp}/pseudo_fold{fold}_ids.npy\")\n",
    "        pseudo_mask = np.load(f\"../output/exp/ex{pseudo_exp}/pseudo_fold{fold}_attention.npy\")\n",
    "        pseudo_type = np.load(f\"../output/exp/ex{pseudo_exp}/pseudo_fold{fold}_type.npy\")\n",
    "        \n",
    "        use_idx = np.random.choice(np.arange(len(pseudo_label)), pseudo_size,replace=False)\n",
    "\n",
    "        pseudo_label = pseudo_label[use_idx]\n",
    "        pseudo_ids = pseudo_ids[use_idx]\n",
    "        pseudo_mask = pseudo_mask[use_idx]\n",
    "        pseudo_type = pseudo_type[use_idx]\n",
    "        \n",
    "        x_train_ids = ids_seq[train[\"fold\"] != fold]\n",
    "        x_train_mask = mask_seq[train[\"fold\"] != fold]\n",
    "        x_train_type = token_type_seq[train[\"fold\"] != fold]\n",
    "        x_train_label = labels_seq[train[\"fold\"] != fold]\n",
    "        \n",
    "        x_train_ids = np.concatenate([x_train_ids, pseudo_ids],axis=0)\n",
    "        x_train_mask = np.concatenate([x_train_mask, pseudo_mask],axis=0)\n",
    "        x_train_type = np.concatenate([x_train_type , pseudo_type],axis=0)\n",
    "        x_train_label = np.concatenate([x_train_label , pseudo_label],axis=0)\n",
    "        \n",
    "        x_val_ids = ids_seq[train[\"fold\"] == fold]\n",
    "        x_val_mask = mask_seq[train[\"fold\"] == fold]\n",
    "        x_val_type = token_type_seq[train[\"fold\"] == fold]\n",
    "        x_val_label = labels_seq[train[\"fold\"] == fold]\n",
    "        \n",
    "        x_val =  train.loc[train[\"fold\"] == fold].reset_index(drop=True)\n",
    "        valid_texts = x_val['pn_history'].values\n",
    "        valid_labels = create_labels_for_scoring(x_val)\n",
    "        \n",
    "        # dataset\n",
    "        #train_ = TrainDataset(x_train,tokenizer)\n",
    "        train_aug = TrainDataset_pseudo_aug(x_train_ids, \n",
    "                                            x_train_mask,\n",
    "                                            x_train_type,\n",
    "                                            x_train_label,\n",
    "                                            tokenizer,0.5,0.15)\n",
    "        val_ = TrainDataset_pseudo(x_val_ids, \n",
    "                            x_val_mask,\n",
    "                            x_val_type,\n",
    "                            x_val_label,\n",
    "                            tokenizer)\n",
    "        \n",
    "        # loader\n",
    "        train_loader = DataLoader(dataset=train_aug, batch_size=BATCH_SIZE, shuffle = True ,pin_memory=True)\n",
    "        #train_aug_loader = DataLoader(dataset=train_aug, batch_size=BATCH_SIZE, shuffle = True ,pin_memory=True)\n",
    "        val_loader = DataLoader(dataset=val_, batch_size=BATCH_SIZE, shuffle = False , pin_memory=True)\n",
    "        \n",
    "        # model\n",
    "        model = custom_model()\n",
    "        model.load_state_dict(torch.load(PRETRAIN_MODEL_PATH),strict=False)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # optimizer, scheduler\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=lr,\n",
    "                          betas=beta,\n",
    "                          weight_decay=weight_decay,\n",
    "                          )\n",
    "        num_train_optimization_steps = int(len(train_loader) * n_epochs / iters_to_accumulate)\n",
    "        num_warmup_steps = int(num_train_optimization_steps * num_warmup_steps_rate)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=num_warmup_steps,\n",
    "                                                    num_training_steps=num_train_optimization_steps)\n",
    "        \n",
    "        criterion = SmoothFocalLoss(reduction=\"mean\", alpha=1, gamma=2, smoothing=0.0001)\n",
    "        best_val = 0\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            print(f\"============start epoch:{epoch}============\")\n",
    "            model.train() \n",
    "            val_losses_batch = []\n",
    "            scaler = GradScaler()\n",
    "            for i, d in tqdm(enumerate(train_loader),total=len(train_loader)):\n",
    "                d = collate(d)\n",
    "                ids = d['token'].to(device)\n",
    "                mask = d['mask'].to(device)\n",
    "                token_type = d[\"token_type\"].to(device)\n",
    "                labels = d['y'].to(device)\n",
    "                labels = labels.unsqueeze(-1)\n",
    "                #labels = labels.unsqueeze(-1)\n",
    "                #optimizer.zero_grad()\n",
    "                with autocast():\n",
    "                    output = model(ids,mask,token_type)\n",
    "                    loss = criterion(output[labels != -1], labels[labels != -1])\n",
    "                    loss = loss / iters_to_accumulate\n",
    "                scaler.scale(loss).backward()\n",
    "                if (i + 1) % iters_to_accumulate == 0:\n",
    "                    # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n",
    "\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    scheduler.step()\n",
    "                if (i + 1) == eval_step:\n",
    "                    val_preds = np.ndarray((0,max_len,1))\n",
    "                    model.eval()  # switch model to the evaluation mode\n",
    "                    with torch.no_grad():  \n",
    "                        for d in tqdm(val_loader,total=len(val_loader)):\n",
    "                            # =========================\n",
    "                            # data loader\n",
    "                            # =========================\n",
    "                            d = collate(d)\n",
    "                            ids = d['token'].to(device)\n",
    "                            mask = d['mask'].to(device)\n",
    "                            token_type = d[\"token_type\"].to(device)\n",
    "                            with autocast():\n",
    "                                outputs = model(ids, mask,token_type)\n",
    "                            outputs = np.concatenate([outputs.sigmoid().detach().cpu().numpy(),np.zeros([len(outputs),max_len - d[\"max_len\"],1])],axis=1)\n",
    "                            val_preds = np.concatenate([val_preds, outputs], axis=0)\n",
    "                    char_probs = get_char_probs(valid_texts, val_preds,  tokenizer)\n",
    "                    results = get_results(char_probs,th=0.5)\n",
    "                    preds = get_predictions(results)\n",
    "                    score,re_score = get_score(valid_labels, preds)\n",
    "                    LOGGER.info(f'{fold},{epoch}:val_score:{score}')\n",
    "                    if best_val < score:\n",
    "                        print(\"save model weight\")\n",
    "                        best_val = score\n",
    "                        best_val_preds = val_preds\n",
    "                        torch.save(model.state_dict(), MODEL_PATH_BASE + f\"_{fold}.pth\") # Saving current best model\n",
    "                    model.train()\n",
    "            \n",
    "            val_preds = np.ndarray((0,max_len,1))\n",
    "            model.eval()  # switch model to the evaluation mode\n",
    "            with torch.no_grad():  \n",
    "                for d in tqdm(val_loader,total=len(val_loader)):\n",
    "                    # =========================\n",
    "                    # data loader\n",
    "                    # =========================\n",
    "                    d = collate(d)\n",
    "                    ids = d['token'].to(device)\n",
    "                    mask = d['mask'].to(device)\n",
    "                    token_type = d[\"token_type\"].to(device)\n",
    "                    with autocast():\n",
    "                        outputs = model(ids, mask,token_type)\n",
    "                    outputs = np.concatenate([outputs.sigmoid().detach().cpu().numpy(),np.zeros([len(outputs),max_len - d[\"max_len\"],1])],axis=1)\n",
    "                    val_preds = np.concatenate([val_preds, outputs], axis=0)\n",
    "            char_probs = get_char_probs(valid_texts, val_preds,  tokenizer)\n",
    "            results = get_results(char_probs,th=0.5)\n",
    "            preds = get_predictions(results)\n",
    "            score,re_score = get_score(valid_labels, preds)\n",
    "            LOGGER.info(f'{fold},{epoch}:val_score:{score}')\n",
    "            if best_val < score:\n",
    "                print(\"save model weight\")\n",
    "                best_val = score\n",
    "                best_val_preds = val_preds\n",
    "                torch.save(model.state_dict(), MODEL_PATH_BASE + f\"_{fold}.pth\") # Saving current best model\n",
    "        oof_pred[train[\"fold\"] == fold,:] = best_val_preds.reshape([-1,max_len])\n",
    "        np.save(OUTPUT_DIR + f\"/ex{ex}_oof_npy_{fold}.npy\",best_val_preds)\n",
    "    # cv\n",
    "    valid_texts = train['pn_history'].values\n",
    "    valid_labels = create_labels_for_scoring(train)\n",
    "    char_probs = get_char_probs(valid_texts, oof_pred,  tokenizer)\n",
    "    results = get_results(char_probs, th=0.5)\n",
    "    preds = get_predictions(results)\n",
    "    score = get_score(valid_labels, preds)\n",
    "    LOGGER.info(f'cv:{score}')\n",
    "    np.save(OUTPUT_DIR + f\"/ex{ex}_oof.npy\",oof_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f60415b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
